{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A  | Statistical tests and comparing groups\n",
    "### Spring 2023, Week 3\n",
    "<hr>\n",
    "\n",
    "Objectives:\n",
    "-  Run and interpret a t-test\n",
    "-  Understand p-values and common pitfalls\n",
    "-  Gain intuition about statistical tests and sample sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of bash commands to make google colab work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/BIOL359A-FoundationsOfQBio-Spr23/week3_statisticaltests\n",
    "!mkdir ./data\n",
    "!cp week3_statisticaltests/data/* ./data\n",
    "!cp week3_statisticaltests/clean_data.py ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements\n",
    "\n",
    "Import statements are used to integrate *external code or packages* into our analysis. \n",
    "\n",
    "- `pandas`: Represents data as tables\n",
    "- `seaborn`: Data exploration visualization tool\n",
    "- `ipywidgets`: Notebook widgets that add user interfaces to notebooks\n",
    "- `random`: Generate random numbers\n",
    "- `numpy`: General math/matrices package\n",
    "- `matplotlib`: Data visualization software\n",
    "- `Scipy`: General scientific computing\n",
    "\n",
    "Using `as` will alias (rename) the package in the code.\n",
    "`matplotlib.pyplot` is importing the submodule `pyplot` from `matplotlib`. \n",
    "`from scipy.stats` is telling python where to find `ttest_ind`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "\n",
    "TITLE_FONT = 20\n",
    "LABEL_FONT = 16\n",
    "TICK_FONT = 16\n",
    "FIG_SIZE = (12,12)\n",
    "COLORS= [\"#008080\",\"#CA562C\"]\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=1) #Change from 1 to 1.5 or 2 if you have a hard time reading text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing and the null distribution:\n",
    "\n",
    "One of the goals of this classwork is to reinforce the relationship between the null hypothesis, null distribution, p-values, and hypothesis tests. To review, the standard procedure of a hypothesis test is as follows:\n",
    "\n",
    "1) define your __Null Hypothesis__, or what you'd believe based on no evidence. This hypothesis is usually the \"lack of an effect\" which can look like $\\mathcal{H}_0: \\bar{X} = \\mu$. Your __alternative hypothesis__ is usually \"everything except the null hypothesis\", which looks like $\\mathcal{H}_A: \\bar{X} \\neq \\mu$. Other common alternatives are $\\mathcal{H}_A: \\bar{X} > \\mu$ or $\\mathcal{H}_A: \\bar{X} < \\mu$. Which one you choose effects what your __critical values__ are. \n",
    "\n",
    "2) Based on the null hypothesis, you are looking for a __Test Statistic__, a random variable that captures your null hypothesis. For a null hypothesis of $\\mathcal{H}_0: \\bar{X} = \\mu$, the common test statistic that assumes an unknown variance is _t_ where $s_x$ is the sample variance:\n",
    "\n",
    "$$ t = \\frac{\\bar{X} - \\mu}{s_x/\\sqrt{n}} $$\n",
    "\n",
    "3) The distribution of your test statistic is called your __Null Distribution__. The distribution of the _t_ statistic  is the _t_-distribution, which we will look at a little bit more throughout this classwork. Notice that assuming the null hypothesis is true ($\\bar{X} = \\mu$) means that _t_ is equal to 0, and the mean of the _t_-distribution is 0. \n",
    "\n",
    "4) The purpose of the test is to determine how likely/unlikely it is to observe your sample in context of the assumptions stated above. In order to do so, we will find $\\bar{X}$ on the null distribution. The probability of $\\bar{X}$ occuring is the ___p_-value__. Whether or not you label something as significant is by comparing it to your __significance level__, $\\alpha$.\n",
    "\n",
    "![pvalue.png](pvalue.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alpha.png](alpha.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classwork starts here:\n",
    "\n",
    "# Question 1: Introduce yourself to your group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can ignore the code below! It is setting up an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_datasets(n, rng, repetitions=200):\n",
    "    \"\"\"Generate (repititions) of sample averages, based on (n) samples per average\"\"\"\n",
    "    averages = []\n",
    "    all_nums = []\n",
    "    for i in range(0, repetitions):\n",
    "        nums = [generate_random_numbers(rng) for _ in range(0,n)]\n",
    "        all_nums += nums\n",
    "        averages.append(np.mean(nums))\n",
    "    return all_nums, averages\n",
    "    \n",
    "def generate_histograms_clt(axs, n=10, rng=\"uniform\"):\n",
    "    \"\"\"build 2x2 matrix of histograms\"\"\"\n",
    "\n",
    "    all_nums_fixed, averages_fixed = get_random_datasets(10, rng)\n",
    "    build_paired_histograms(averages_fixed, all_nums_fixed, 10, rng, axs, column=0)\n",
    "    \n",
    "    all_nums, averages = get_random_datasets(n, rng)\n",
    "    build_paired_histograms(averages, all_nums, n, rng, axs, column=1)\n",
    "    \n",
    "def build_paired_histograms(averages, all_nums, n, rng, axs, column):\n",
    "    \"\"\"histogram plotting specific to this lecture\"\"\"\n",
    "    colors=[\"#1e81b0\", \"#e28743\"]\n",
    "    color = colors[column]\n",
    "    axs[0,column].set_title(f\"random samples\")\n",
    "    axs[1,column].set_title(f\"sample averages\")\n",
    "    axs[0,column].text(0.9, 0.9, f\"n:{n}\"+r\" $\\mu$: {0:.2f}, $\\sigma$:{1:.2f}\".format(np.mean(all_nums), np.std(all_nums)) ,\n",
    "                       verticalalignment='bottom', horizontalalignment='right',\n",
    "                       transform=axs[0,column].transAxes,\n",
    "                       color=color, fontsize=LABEL_FONT)\n",
    "    axs[1,column].text(0.9, 0.9, f\"n:{n}\"+r\" $\\hat{{\\mu}}_\\bar{{X}}$: {0:.2f}, $\\hat{{\\sigma}}_\\bar{{X}}$:{1:.2f}\".format(np.mean(averages), np.std(averages)),\n",
    "                       verticalalignment='bottom', horizontalalignment='right',\n",
    "                       transform=axs[1,column].transAxes,\n",
    "                       color=color, fontsize=LABEL_FONT)\n",
    "    sns.histplot(all_nums, ax=axs[0, column], color = color, stat=\"probability\", kde=True)\n",
    "    sns.histplot(averages, bins=10, ax=axs[1, column], color = color, stat=\"probability\", kde=True)\n",
    "    axs[1, column].set_xlim(0,10)\n",
    "    \n",
    "    \n",
    "def generate_random_numbers(generator = \"uniform\"):\n",
    "    \"\"\"generate random numbers with a mean of 5\"\"\"\n",
    "    if generator == \"uniform\": return random.uniform(0,10)\n",
    "    elif generator == \"exponential\": return random.expovariate(1/5)\n",
    "    elif generator == \"normal\": return random.gauss(5,2)\n",
    "    \n",
    "def format_plots(axs):\n",
    "    \"\"\"some extra formatting for subplots\"\"\"\n",
    "    for ax in axs.flat:\n",
    "        title = ax.get_title()\n",
    "        ax.set_title(title, fontweight=\"bold\", size=LABEL_FONT)\n",
    "        ax.set_ylabel('Proportion (Probability)', fontsize = LABEL_FONT)\n",
    "        ax.set_xlabel('Number', fontsize = LABEL_FONT)\n",
    "        ax.tick_params(labelsize=TICK_FONT)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context: Understanding our data\n",
    "\n",
    "We will be using the same tumor dataset we looked at last week. The python script is loading the file and doing some basic cleaning of parts of the dataset we aren't using. It can be found in `clean_data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean_data #helper function with \n",
    "\n",
    "cancer_dataset = clean_data.generate_clean_dataframe()\n",
    "cancer_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A reminder on the dataset\n",
    "\n",
    "From the data source: Wisconsin Diagnostic Breast Cancer (WDBC)\n",
    "\n",
    "```\n",
    "\tFeatures are computed from a digitized image of a fine needle\n",
    "\taspirate (FNA) of a breast mass.  They describe\n",
    "\tcharacteristics of the cell nuclei present in the image.\n",
    "\tA few of the images can be found at\n",
    "\thttp://www.cs.wisc.edu/~street/images/\n",
    "\n",
    "\tSeparating plane described above was obtained using\n",
    "\tMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
    "\tConstruction Via Linear Programming.\" Proceedings of the 4th\n",
    "\tMidwest Artificial Intelligence and Cognitive Science Society,\n",
    "\tpp. 97-101, 1992], a classification method which uses linear\n",
    "\tprogramming to construct a decision tree.  Relevant features\n",
    "\twere selected using an exhaustive search in the space of 1-4\n",
    "\tfeatures and 1-3 separating planes.\n",
    "\n",
    "\tThe actual linear program used to obtain the separating plane\n",
    "\tin the 3-dimensional space is that described in:\n",
    "\t[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
    "\tProgramming Discrimination of Two Linearly Inseparable Sets\",\n",
    "\tOptimization Methods and Software 1, 1992, 23-34].\n",
    "    \n",
    "    Source:\n",
    "    W.N. Street, W.H. Wolberg and O.L. Mangasarian \n",
    "\tNuclear feature extraction for breast tumor diagnosis.\n",
    "\tIS&T/SPIE 1993 International Symposium on Electronic Imaging: Science\n",
    "\tand Technology, volume 1905, pages 861-870, San Jose, CA, 1993.\n",
    "```\n",
    "\n",
    "What do all the column names mean?\n",
    "\n",
    "- ID number\n",
    "- Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry \n",
    "- fractal dimension (\"coastline approximation\" - 1) - a measure of \"complexity\" of a 2D image.\n",
    "\n",
    "\n",
    "Cateogory Distribution: 357 benign, 212 malignant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to show the first five values in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset[\"mean_area\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to show the first five values from the groups in the diagnosis column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset[\"mean_area\"].groupby(\"diagnosis\").head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` has us covered for summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset[\"mean_area\"].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2+3: Formulating hypotheses\n",
    "\n",
    "### Let's split the variables up by their category (also called it's label in data science).\n",
    "\n",
    "Based on our available data, we're not that interested in what the descriptive statistics are on the individual columns. We are interested in the differences between those groups, specifically the benign and malignant tumors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_diagnoses_by_variable(variable: str, dataframe: pd.DataFrame = cancer_dataset):\n",
    "    \"\"\"Accepts column name to generate basic descriptions\"\"\"\n",
    "    df = dataframe.reset_index()\n",
    "    sns.boxplot(data=df, x=variable, y=df[\"diagnosis\"])\n",
    "    return dataframe[variable].groupby(\"diagnosis\").describe()\n",
    "\n",
    "@widgets.interact(variable=list(cancer_dataset))\n",
    "def comparison_wrapper(variable=\"mean_radius\"):\n",
    "    return compare_diagnoses_by_variable(variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run a two-sample t-test on these categories\n",
    "\n",
    "We didn't focus on any specific t-tests, but there are a couple different variations. \n",
    "Often they are introduced for comparing a sample to an assumed value, here $\\mu$. \n",
    " \n",
    "$$ t = \\frac{\\bar{X} - \\mu}{s_x/\\sqrt{n}} $$\n",
    "\n",
    "for the corresponding null hypothesis: $\\mathcal{H}_0: \\bar{X} = \\mu$\n",
    "\n",
    "We often don't know that quantity, but we can compare it to another sample mean. \n",
    "The previous test statistic then becomes\n",
    "\n",
    "$$ t = \\frac{\\bar{X}_1 - \\bar{X}_2}{S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}} $$\n",
    "\n",
    "Where, \n",
    "\n",
    "$$ S_p = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2} $$\n",
    "\n",
    "for the corresponding null hypothesis: $\\mathcal{H}_0: \\bar{X}_1 = \\bar{X}_2$\n",
    "\n",
    "Don't worry we aren't going to ask you to derive, memorize, nor calculate this by hand (even though you can). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ind_ttest(feature=\"mean_radius\", dataset = cancer_dataset, welch=False):\n",
    "    \"\"\"\n",
    "    run two sample t-tests\n",
    "    For the motivated, visit \n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html \n",
    "    to see the documentation to see how to write the code for a t-test.\"\"\"\n",
    "    cat1 = dataset.xs(\"M\", level=1)\n",
    "    cat2 = dataset.xs(\"B\", level=1)\n",
    "    df = dataset.reset_index()\n",
    "    f, (ax_hist, ax_box) = plt.subplots(2, sharex=True)\n",
    "\n",
    "    sns.boxplot(data=df, x=feature, y=df[\"diagnosis\"], ax=ax_box)\n",
    "    sns.histplot(data=df, x=feature, hue=df[\"diagnosis\"], ax=ax_hist, stat=\"probability\", kde=True)\n",
    "    if welch: tstat, pvalue = ttest(cat1[feature], cat2[feature], equal_var=False)\n",
    "    else: tstat, pvalue = ttest(cat1[feature], cat2[feature])\n",
    "    print(f\"p-value: {pvalue:.2e}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use all the data points we have and compare malignant and benign tumors. Choose a variable and predict an outcome, you can then run a t-test to compare the means of the two groups. However, consider if the assumptions for a t-test are accurate (namely, do the variances come from the same sampling population)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(variable=list(cancer_dataset))\n",
    "def run_ttest(variable):\n",
    "    run_ind_ttest(feature=variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This test requires us to assume that the variances from the two samples share a population variance. Practically, if your standard deviations between samples is small (think less than a ratio of 3, but you can be more rigorous with an F-test), you should be okay. In practice, this means that a typical t-test is inappropriate for comparing from two different populations, but a Welch t-test with a slightly different test statistic (that is still from a t-distribution) is used: https://en.wikipedia.org/wiki/Welch%27s_t-test\n",
    "\n",
    "Were there variables that you are uncomfortable making that assumption, based on the variance in the data? We can use a Welch's test that can account for that variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(variable=list(cancer_dataset))\n",
    "def run_welch_test(variable):\n",
    "    run_ind_ttest(feature=variable, welch=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4+5: Sample size and _p_-hacking\n",
    "\n",
    "So we've compared the data based on all of the available data, but what if we were limited in the samples that we were able to collect? We will consider what happens to our comparisons and tests if we are only able to see a subsample of the available data. There are multiple things to consider here but there are two main ideas that drive the concept of _p_-hacking. \n",
    "\n",
    "  1) Intuitively, more samples give you more information about your population. \n",
    "  \n",
    "  2) More samples also reduce your p-values, potentially assigning \"signicance\" to a potentially trivial effect.\n",
    "\n",
    "__Aside__: This process is similar to sampling from the empirical distribution function and is important for concepts like bootstrapping that you may encounter in further readings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@widgets.interact_manual(feature=list(cancer_dataset), n=(3,100))\n",
    "def run_sampled_ttest(feature=\"mean_radius\", n=3):\n",
    "    \"\"\"randomly sampled students t-test with same n\"\"\"\n",
    "    seed = 1\n",
    "    cat1 = cancer_dataset.xs(\"M\", level=1).sample(n, random_state=seed)\n",
    "    cat2 = cancer_dataset.xs(\"B\", level=1).sample(n)\n",
    "    \n",
    "    cat1[\"diagnosis\"] = \"M\"\n",
    "    cat2[\"diagnosis\"] = \"B\"\n",
    "    df = pd.concat([cat1,cat2]).reset_index()\n",
    "    f, (ax_hist, ax_box) = plt.subplots(2, sharex=True)\n",
    "\n",
    "    sns.boxplot(data=df, x=feature, y=df[\"diagnosis\"], ax=ax_box)\n",
    "    sns.histplot(data=df, x=feature, hue=df[\"diagnosis\"], ax=ax_hist, stat=\"probability\", kde=True)    \n",
    "    tstat, pvalue = ttest(cat1[feature], cat2[feature])\n",
    "    print(f\"p-value: {pvalue:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if we are under the impression that the data does not share the population variance, we can use the Welch t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact_manual(feature=list(cancer_dataset), n=(3,100))\n",
    "def run_sampled_welch_ttest(feature=\"mean_radius\", n=3):\n",
    "    \"\"\"randomly sampled welch t-test with same n\"\"\"\n",
    "    seed = 1\n",
    "    cat1 = cancer_dataset.xs(\"M\", level=1).sample(n, random_state=seed)\n",
    "    cat2 = cancer_dataset.xs(\"B\", level=1).sample(n)\n",
    "    \n",
    "    cat1[\"diagnosis\"] = \"M\"\n",
    "    cat2[\"diagnosis\"] = \"B\"\n",
    "    df = pd.concat([cat1,cat2]).reset_index()\n",
    "    f, (ax_hist, ax_box) = plt.subplots(2, sharex=True)\n",
    "\n",
    "    sns.boxplot(data=df, x=feature, y=df[\"diagnosis\"], ax=ax_box)\n",
    "    sns.histplot(data=df, x=feature, hue=df[\"diagnosis\"], ax=ax_hist, stat=\"probability\", kde=True)    \n",
    "    tstat, pvalue = ttest(cat1[feature], cat2[feature], equal_var=False)\n",
    "    print(f\"p-value: {pvalue:.2e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Robustness of the _t_-test\n",
    "\n",
    "### Comparing the pdf of the _t_ distribution and the normal distribution\n",
    "\n",
    "The t-test is a largely robust method of comparing the mean to a value or the mean of another group. The method is generally robust for skewed distributions. Here we are going to compare the t-distribution to the normal distribution. Keeping in mind the CLT, what can we say about the point of diminishing returns with the $t$-distribution? Is there a cutoff point for degrees of freedom? (There is no simple answer here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t, norm\n",
    "\n",
    "@widgets.interact_manual(df=(3,100))\n",
    "def t_vs_norm(df=3):\n",
    "    \"\"\"plot the t-distribution vs the normal distribution pdfs\"\"\"\n",
    "    x = np.linspace(t.ppf(0.01, df), t.ppf(0.99, df), 100)\n",
    "    x = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=FIG_SIZE)\n",
    "    ax.plot(x, t.pdf(x,df),'-', lw=5, alpha=0.6, label='t-distribution pdf')\n",
    "    ax.plot(x, norm.pdf(x),'k:', lw=5, alpha=0.6, label='normal distribution pdf')\n",
    "    plt.ylabel(\"Probability\", fontsize=LABEL_FONT)\n",
    "    plt.legend(loc=\"best\",fontsize=LABEL_FONT)\n",
    "    plt.xlabel(\"X\", fontsize=LABEL_FONT)\n",
    "    plt.title(r\"Student's $t$-Distribution vs. Normal Distribution\", fontsize=TITLE_FONT)\n",
    "    plt.ylim(0,.5)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Relationship between the _t_-distribution and the CLT \n",
    "\n",
    "If you are still confused what the origin of the _t_-distribution is, we will cover that here. \n",
    "This information will not be on an evaluation, but can provide you further intuition about how to think about $t$-tests. \n",
    "\n",
    "We are going to start from the CLT. \n",
    "We know that the sample means of a population are normally distributed, and therefore if we design our test-statistic with that in mind, we can some of the nice properties of the normal distribution. \n",
    "Although we simplified the CLT equation, it is more common to see it written as: \n",
    "\n",
    "\n",
    "$$\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow[]{d} N(0, 1)$$\n",
    "\n",
    "Meaning that the left hand side of the equation will converge on the standard normal, or the normal distribution with a mean of 0 and a standard deviation of 1. \n",
    "\n",
    "Does the left hand side look familiar? Compare it to the original t-test statistic. \n",
    "\n",
    "$$ t = \\frac{\\bar{X} - \\mu}{s_x/\\sqrt{n}} $$\n",
    "\n",
    "Because we generally don't know the $\\sigma$ of the underlying distribution (in general: we are trying to understand the population from a sample, not understand a sample from a population), we need to estimate $\\sigma$ with the sample standard deviation, $s_x$. \n",
    "Similarly to the CLT, we know that the the sample deviation has a chi-squared distribution, $\\chi^2$. \n",
    "This fact leads to the t statistic being the normal distribution, divided by the chi-squared distribution, yielding the $t$-distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
